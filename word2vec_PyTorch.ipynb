{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install pandas\n",
    "! pip install numpy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gensim\n",
    "import gensim.downloader\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only run this cell if you want to generate a new validation set\n",
    "df = pd.read_csv(\"trec/original/train.csv\")\n",
    "# TODO: check w group if we need this\n",
    "# Remove duplicates from train\n",
    "df.drop_duplicates(subset='text', keep='first', inplace=True)\n",
    "# Randomly choose 500 rows to drop \n",
    "num_rows_to_drop = 500\n",
    "np.random.seed(42)\n",
    "rows_to_drop = np.random.choice(df.index, num_rows_to_drop, replace=False)\n",
    "# print(rows_to_drop)\n",
    "# Create a development dataframe from these 500 dropped rows\n",
    "validation_df = df.loc[rows_to_drop].copy()\n",
    "\n",
    "# Reset index of development dataframe and export to csv\n",
    "validation_df.reset_index(drop=True, inplace=True)\n",
    "validation_df.to_csv(\"trec/generated/validation.csv\",index=None)\n",
    "\n",
    "df_copy = df.copy(deep=True)\n",
    "# Drop validation rows from original dataset, export as csv\n",
    "df_copy.drop(rows_to_drop, inplace=True)\n",
    "df_copy.reset_index(drop=True, inplace=True)\n",
    "df_copy.to_csv(\"trec/generated/train.csv\",index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell if you're generating a new validation set for sanity checking\n",
    "def check_unique_texts(train_csv_file, validation_csv_file):\n",
    "    train_df = pd.read_csv(train_csv_file)\n",
    "    validation_df = pd.read_csv(validation_csv_file)\n",
    "\n",
    "    train_texts = train_df['text']\n",
    "    validation_texts = validation_df['text']\n",
    "\n",
    "    common_texts = validation_texts[validation_texts.isin(train_texts)]\n",
    "\n",
    "    if common_texts.empty:\n",
    "        print(\"Validation set and train sets are unique\")\n",
    "    else:\n",
    "        print(\"Common values found in the 'text' column:\")\n",
    "        print(common_texts)\n",
    "\n",
    "train_csv_file = \"trec/generated/train.csv\"\n",
    "validation_csv_file = \"trec/generated/validation.csv\"\n",
    "check_unique_texts(train_csv_file, validation_csv_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for duplicates from test - remove if there are any\n",
    "df = pd.read_csv(\"trec/original/test.csv\")\n",
    "df.drop_duplicates(subset='text', keep='first', inplace=True)\n",
    "df.to_csv('trec/generated/test.csv',index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df=pd.read_csv('trec/generated/train.csv')\n",
    "val_df=pd.read_csv('trec/generated/validation.csv')\n",
    "test_df=pd.read_csv('trec/generated/test.csv')\n",
    "\n",
    "train_df.drop(columns='label-fine', inplace=True)\n",
    "val_df.drop(columns='label-fine', inplace=True)\n",
    "test_df.drop(columns='label-fine', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes=train_df['label-coarse'].unique()\n",
    "random.shuffle(classes)\n",
    "for i in classes[:2]:\n",
    "    train_df['label-coarse']=train_df['label-coarse'].apply(lambda x:'OTHERS' if x==i else x)\n",
    "    val_df['label-coarse']=val_df['label-coarse'].apply(lambda x:'OTHERS' if x==i else x)\n",
    "    test_df['label-coarse']=test_df['label-coarse'].apply(lambda x:'OTHERS' if x==i else x)\n",
    "\n",
    "mapping_dict = {item: idx for idx, item in enumerate(set(train_df['label-coarse'].unique()))}\n",
    "train_df['label-coarse']=train_df['label-coarse'].apply(lambda x:mapping_dict[x])\n",
    "val_df['label-coarse']=val_df['label-coarse'].apply(lambda x:mapping_dict[x])\n",
    "test_df['label-coarse']=test_df['label-coarse'].apply(lambda x:mapping_dict[x])\n",
    "\n",
    "train_df['label-coarse'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_model = gensim.downloader.load('word2vec-google-news-300')\n",
    "\n",
    "def text_to_word2vec(text, word2vec_model):\n",
    "    #words = nltk.word_tokenize(text)\n",
    "    words=text.split(\" \")\n",
    "    embeddings = []\n",
    "    for word in words:\n",
    "        if word in word2vec_model:\n",
    "            embeddings.append(word2vec_model[word])\n",
    "        else:\n",
    "            #If word not in the vocab, use a default vector or zeros\n",
    "            embeddings.append(np.zeros(word2vec_model.vector_size, dtype=np.float32))\n",
    "            #pass\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data, max_seq_length,word2vec_model):\n",
    "        self.data = data\n",
    "        self.word2vec_model=word2vec_model\n",
    "        self.max_seq_length = max_seq_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        label,text = self.data.iloc[idx]\n",
    "        text = text_to_word2vec(text, self.word2vec_model)\n",
    "\n",
    "        text = np.array(text, dtype=np.float32)\n",
    "        if text.shape[0] < self.max_seq_length:\n",
    "            padding = np.zeros((self.max_seq_length - text.shape[0], text.shape[1]), dtype=np.float32)\n",
    "            text = np.vstack((text, padding))\n",
    "        elif text.shape[0] > self.max_seq_length:\n",
    "            text = text[:self.max_seq_length]\n",
    "        return torch.Tensor(text), label\n",
    "    \n",
    "def intialise_loaders(df,max_seq_length,batch_size):\n",
    "    dataloader = DataLoader(CustomDataset(df,max_seq_length,word2vec_model), batch_size=batch_size, shuffle=True)\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AveragePooling(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool1d(1),  # Average pooling over the sequence\n",
    "            nn.Flatten(),  # Flatten the tensor\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, output_size),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply permute operation before feeding to the sequential layers\n",
    "        x = x.permute(0, 2, 1)\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaxPooling(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.AdaptiveMaxPool1d(1),  # Max pooling over the sequence\n",
    "            nn.Flatten(),  # Flatten the tensor\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, output_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply permute operation before feeding to the sequential layers\n",
    "        x = x.permute(0, 2, 1)\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size) # LSTM layer\n",
    "        self.fc = nn.Linear(hidden_size, output_size) # Linear layer for classification\n",
    "\n",
    "    def forward(self, x):\n",
    "        lstm_out, _ = self.lstm(x) # Pass through the LSTM\n",
    "        last_hidden_state = lstm_out[:, -1, :] # Take the output from the last time step\n",
    "        output = self.fc(last_hidden_state) # Apply the linear layer for classification\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopper:\n",
    "    def __init__(self, patience=5, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.min_validation_loss = np.inf\n",
    "\n",
    "    def early_stop(self, validation_loss):\n",
    "        if validation_loss < self.min_validation_loss:\n",
    "            self.min_validation_loss = validation_loss\n",
    "            self.counter = 0\n",
    "        elif validation_loss > (self.min_validation_loss + self.min_delta):\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                return True\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch % 100 == 0: \n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "    return test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length = 50\n",
    "input_size = word2vec_model.vector_size\n",
    "output_size = len(mapping_dict)\n",
    "epochs=100\n",
    "\n",
    "hyperparameter_grid = {\n",
    "    'hidden_size': [8,16,32,64,128,256,512],\n",
    "    'batch_size': [8,16, 32, 64]\n",
    "}\n",
    "\n",
    "best_loss = np.Inf\n",
    "best_hyperparameters = None\n",
    "\n",
    "# Iterate over the parameter grid\n",
    "for params in ParameterGrid(hyperparameter_grid):\n",
    "    early_stopper=EarlyStopper()\n",
    "    \n",
    "    hidden_size = params['hidden_size']\n",
    "    batch_size = params['batch_size']\n",
    "\n",
    "    # Initialize and train your model using the current hyperparameters\n",
    "    #model = AveragePooling(input_size, hidden_size, output_size)\n",
    "    #model = MaxPooling(input_size, hidden_size, output_size)\n",
    "    model = LSTM(input_size, hidden_size, output_size)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "    # Training and validation data loaders\n",
    "    train_dataloader = intialise_loaders(train_df, max_seq_length, batch_size)\n",
    "    val_dataloader = intialise_loaders(val_df, max_seq_length, batch_size)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        train(train_dataloader, model, criterion, optimizer)\n",
    "        val_loss = test(val_dataloader, model, criterion)\n",
    "        if early_stopper.early_stop(val_loss):\n",
    "            print(f'Early Stopping at epoch {epoch+1}')\n",
    "            break\n",
    "\n",
    "    # Evaluate the model on the validation set\n",
    "    if val_loss < best_loss:\n",
    "        best_loss = val_loss\n",
    "        best_hyperparameters = params\n",
    "\n",
    "print(\"Best Hyperparameters:\", best_hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_hidden_size = best_hyperparameters['hidden_size']\n",
    "# best_batch_size = best_hyperparameters['batch_size']\n",
    "best_hidden_size =512\n",
    "best_batch_size =32\n",
    "\n",
    "best_loss = np.Inf\n",
    "\n",
    "#best_model = AveragePooling(input_size, best_hidden_size, output_size)\n",
    "#best_model = MaxPooling(input_size, best_hidden_size, output_size)\n",
    "best_model = LSTM(input_size, best_hidden_size, output_size)\n",
    "\n",
    "\n",
    "print(best_model)\n",
    "train_dataloader = intialise_loaders(train_df, max_seq_length, best_batch_size)\n",
    "val_dataloader = intialise_loaders(val_df, max_seq_length, best_batch_size)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(best_model.parameters(), lr=0.001)\n",
    "\n",
    "early_stopper=EarlyStopper()\n",
    "for epoch in range(epochs):\n",
    "    train(train_dataloader, best_model, criterion, optimizer)\n",
    "    val_loss = test(val_dataloader, best_model, criterion)\n",
    "    if early_stopper.early_stop(val_loss):\n",
    "        print(f'Early Stopping at epoch {epoch+1}')\n",
    "        break\n",
    "\n",
    "test_dataloader = intialise_loaders(test_df, max_seq_length, best_batch_size)\n",
    "test(test_dataloader, best_model, criterion)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test accuracies:\n",
    "- 84% avg pooling\n",
    "- 72.6% max pooling"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
